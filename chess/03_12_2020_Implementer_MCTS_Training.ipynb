{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-12-2020 Implementer MCTS Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIBPT8Pw5EIhYMly0goRSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/entruv/code_stream/blob/master/chess/03_12_2020_Implementer_MCTS_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSSEoMCBFzOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sx_b4VrF6SW",
        "colab_type": "text"
      },
      "source": [
        "## Entrainer MCTSAI\n",
        "\n",
        "- Importer la classe  MCTSAI qui correspond a l'arbre\n",
        "- Importer la classe BoardAi qui correspond aux noeuds\n",
        "- Importer le modele "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9R1GCM-HNPt",
        "colab_type": "text"
      },
      "source": [
        "### Helpers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjM9aBQgHOp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_uci_labels():\n",
        "    \"\"\"\n",
        "    Creates the labels for the universal chess interface into an array and returns them\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    labels_array = []\n",
        "    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] #indice colonnes echiquier\n",
        "    numbers = ['1', '2', '3', '4', '5', '6', '7', '8'] #indice lignes echiquier\n",
        "    promoted_to = ['q', 'r', 'b', 'n'] #list of the available promotion for the pawn\n",
        "\n",
        "    for l1 in range(8):\n",
        "        for n1 in range(8):\n",
        "            destinations = [(t, n1) for t in range(8)] + \\\n",
        "                           [(l1, t) for t in range(8)] + \\\n",
        "                           [(l1 + t, n1 + t) for t in range(-7, 8)] + \\\n",
        "                           [(l1 + t, n1 - t) for t in range(-7, 8)] + \\\n",
        "                           [(l1 + a, n1 + b) for (a, b) in\n",
        "                            [(-2, -1), (-1, -2), (-2, 1), (1, -2), (2, -1), (-1, 2), (2, 1), (1, 2)]]\n",
        "            for (l2, n2) in destinations:\n",
        "                if (l1, n1) != (l2, n2) and l2 in range(8) and n2 in range(8):\n",
        "                    move = letters[l1] + numbers[n1] + letters[l2] + numbers[n2]\n",
        "                    labels_array.append(move)\n",
        "    for l1 in range(8):\n",
        "        l = letters[l1]\n",
        "        for p in promoted_to:\n",
        "            labels_array.append(l + '2' + l + '1' + p)\n",
        "            labels_array.append(l + '7' + l + '8' + p)\n",
        "            if l1 > 0:\n",
        "                l_l = letters[l1 - 1]\n",
        "                labels_array.append(l + '2' + l_l + '1' + p)\n",
        "                labels_array.append(l + '7' + l_l + '8' + p)\n",
        "            if l1 < 7:\n",
        "                l_r = letters[l1 + 1]\n",
        "                labels_array.append(l + '2' + l_r + '1' + p)\n",
        "                labels_array.append(l + '7' + l_r + '8' + p)\n",
        "    return labels_array\n",
        "all_moves = create_uci_labels()\n",
        "all_moves_dict_str = {move: i for i, move in enumerate(all_moves)}\n",
        "all_moves_dict_index = {i: move for i, move in enumerate(all_moves)}\n",
        "## passer d'indices a coup et de coup a indices\n",
        "def get_move_index(move_str):\n",
        "  #renvoie l'index d'un coup sous forme de charactere\n",
        "  return all_moves_dict_str[move_str]\n",
        "\n",
        "def get_move_str(move_index):\n",
        "  # renvoie le coup en charactere a partir de l'indice du coup\n",
        "  return all_moves_dict_index[move_index]\n",
        "\n",
        "\n",
        "\n",
        "def proba_to_policy(node, probabilities):\n",
        "  # projecte la probabilite des coups du modele vers la sous liste des coup possibles,\n",
        "  # en renormalisant le tout\n",
        "  probabilities = np.zeros(len(all_moves))\n",
        "  legal_moves = node.legal_moves()\n",
        "  policy = {}\n",
        "  for move in legal_moves:\n",
        "    policy[move] = probabilities[get_move_index(str(move))]\n",
        "assert get_move_str(32) == get_move_str(get_move_index('a2a4')) #big test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGkoJA4PG773",
        "colab_type": "text"
      },
      "source": [
        "### BOARDAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyAIQkLSGrPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import copy #permet de copier un objet proprement\n",
        "import chess \n",
        "\n",
        "class BoardAi(chess.Board): #Node\n",
        "    \"\"\"\n",
        "    A representation of a single board state.\n",
        "    MCTS works by constructing a tree of these Nodes.\n",
        "    Could be e.g. a chess or checkers board state.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_last_move(self):\n",
        "      #renvoie le dernier coup de cette position\n",
        "      return str(self.move_stack[-1]) if board.move_stack else \"\"\n",
        "    \n",
        "\n",
        "    def get_numpy_board(self):\n",
        "      def transform_line(line):\n",
        "          result = \"\"\n",
        "          for character in line:\n",
        "              if character.isdigit():\n",
        "                  for i in range(int(character)):\n",
        "                      result += \"1\"\n",
        "              else:\n",
        "                  result += character\n",
        "          return result\n",
        "      \n",
        "      def compute_en_passant(ep_square):\n",
        "          def alg_to_coord(alg):\n",
        "              rank = 8 - int(alg[1])        # 0-7\n",
        "              file = ord(alg[0]) - ord('a') # 0-7 #WTF ord(\"a\")??\n",
        "              return rank, file\n",
        "          # renvoie une matrice de la taille dun jeu d'echec, contenant True si la case possede un \n",
        "          #pion que l'on peut manger en passant\n",
        "          en_passant = np.zeros((8, 8, 1), dtype=np.bool)\n",
        "          if ep_square != '-':\n",
        "              eps = alg_to_coord(ep_square)\n",
        "              en_passant[eps[0]][eps[1]] = True\n",
        "          return en_passant\n",
        "\n",
        "      fen_data = self.fen().split(\" \")\n",
        "      fen, turn, castling, ep_square, half_move_clock, fullmove_number = fen_data\n",
        "      en_passant = compute_en_passant(ep_square)\n",
        "      auxiliary_planes = [np.full((8, 8, 1), ('K' in castling), dtype=np.bool),\n",
        "                          np.full((8, 8, 1), ('Q' in castling), dtype=np.bool),\n",
        "                          np.full((8, 8, 1), ('k' in castling), dtype=np.bool),\n",
        "                          np.full((8, 8, 1), ('q' in castling), dtype=np.bool),\n",
        "                          en_passant]\n",
        "      \n",
        "      piece_information_plane = np.zeros((8,8, len(one_hot_dictionary)))\n",
        "      for i, line in enumerate(fen.split(\"/\")):\n",
        "          for j, case in enumerate(transform_line(line)):\n",
        "              piece_information_plane[i][j] = one_hot_dictionary[case]    \n",
        "      all_planes = auxiliary_planes + [piece_information_plane]\n",
        "      all_planes_numpy = np.concatenate(all_planes, axis=-1).astype(np.float32)\n",
        "      return all_planes_numpy\n",
        "\n",
        "    def find_legal_moves(self):\n",
        "      return list(self.legal_moves)\n",
        "    \n",
        "    def get_children(self, move_str):\n",
        "      board = self.copy()\n",
        "      board.push(move)\n",
        "      return board\n",
        "  \n",
        "    def find_children(self):\n",
        "      \"All possible successors of this board state\"\n",
        "      childrens = []\n",
        "      for move in self.find_legal_moves():\n",
        "          board = self.copy()\n",
        "          board.push(move)\n",
        "          childrens.append(board)\n",
        "      return childrens\n",
        "\n",
        "    def find_random_child(self):\n",
        "      childrens = self.find_children()\n",
        "      if childrens:\n",
        "        return np.random.choice(childrens)\n",
        "        \"Random successor of this board state (for more efficient simulation)\"\n",
        "      return None\n",
        "\n",
        "    def is_terminal(self):\n",
        "      # use the 50 move rule to stop the game accordingly\n",
        "      return len(self.find_children()) == 0 or self.can_claim_draw()\n",
        "\n",
        "\n",
        "    def get_reward(self, result):\n",
        "      if result == \"1-0\":\n",
        "          return 1\n",
        "      if result == \"0-1\":\n",
        "          return -1\n",
        "      if result == \"1/2-1/2\":\n",
        "          return 0 #biais pour explorer les parties qui ne rendent pas un etat null\n",
        "      return 0\n",
        "    def reward(self):\n",
        "      \"Assumes `self` is terminal node.\"\n",
        "      result = self.result() #vient de la classe parent chess.Board()\n",
        "      print(f\"Resultat de la partie: {result}\")\n",
        "      return self.get_reward(result)\n",
        "\n",
        "    def identifier(self):\n",
        "      return str(self.board_fen()) + str(self.turn) + str(self.castling_rights) + str(self.ep_square)\n",
        "    \n",
        "    def __hash__(self):\n",
        "        \"Nodes must be hashable\"\n",
        "        #permet de distinguer si deux noeuds sont identiques ou differents. Utile car mes noeuds sont des objets pythons difficile a comparer par nature\n",
        "        return hash(self.identifier())\n",
        "    \n",
        "    def __eq__(node1, node2):\n",
        "      return node1.identifier() == node2.identifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEzZcb4G9_d",
        "colab_type": "text"
      },
      "source": [
        "## MCTS AND MCTSAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzcsyFRUG9Io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MCTS:\n",
        "    \"Monte Carlo tree searcher. First rollout the tree then choose a move.\"\n",
        "\n",
        "    def __init__(self, exploration_weight=1):\n",
        "        self.Q = defaultdict(int)  # total reward of each node\n",
        "        self.N = defaultdict(int)  # total visit count for each node\n",
        "        self.children = dict()  # children of each node\n",
        "        self.exploration_weight = exploration_weight\n",
        "\n",
        "    def choose(self, node):\n",
        "        \"Choose the best successor of node. (Choose a move in the game)\"\n",
        "        if node.is_terminal():\n",
        "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
        "\n",
        "        if node not in self.children:\n",
        "            return node.find_random_child()\n",
        "\n",
        "        def score(n):\n",
        "            if self.N[n] == 0:\n",
        "                return float(\"-inf\")  # avoid unseen moves\n",
        "            return self.Q[n] / self.N[n]  # average reward\n",
        "\n",
        "        return max(self.children[node], key=score)\n",
        "\n",
        "    def do_rollout(self, node):\n",
        "        \"Make the tree one layer better. (Train for one iteration.)\"\n",
        "        path = self._select(node)\n",
        "        leaf = path[-1]\n",
        "        self._expand(leaf)\n",
        "        reward = self._simulate(leaf)\n",
        "        self._backpropagate(path, reward)\n",
        "\n",
        "    def _select(self, node):\n",
        "        \"Find an unexplored descendent of `node`\"\n",
        "        path = []\n",
        "        while True:\n",
        "            path.append(node)\n",
        "            if node not in self.children or not self.children[node]:\n",
        "                # node is either unexplored or terminal\n",
        "                return path\n",
        "            unexplored = self.children[node] - self.children.keys()\n",
        "            if unexplored:\n",
        "                n = unexplored.pop()\n",
        "                path.append(n)\n",
        "                return path\n",
        "            node = self._uct_select(node)  # descend a layer deeper\n",
        "\n",
        "    def _expand(self, node):\n",
        "        \"Update the `children` dict with the children of `node`\"\n",
        "        if node in self.children:\n",
        "            return  # already expanded\n",
        "        self.children[node] = node.find_children()\n",
        "\n",
        "    def _simulate(self, node):\n",
        "        \"Returns the reward for a random simulation (to completion) of `node`\"\n",
        "        invert_reward = True\n",
        "        while True:\n",
        "            if node.is_terminal():\n",
        "              reward = node.reward()\n",
        "              return 1 - reward if invert_reward else reward\n",
        "            node = node.find_random_child()\n",
        "            invert_reward = not invert_reward\n",
        "\n",
        "    def _backpropagate(self, path, reward):\n",
        "        \"Send the reward back up to the ancestors of the leaf\"\n",
        "        for node in reversed(path):\n",
        "            self.N[node] += 1\n",
        "            self.Q[node] += reward\n",
        "            reward = 1 - reward  # 1 for me is 0 for my enemy, and vice versa\n",
        "\n",
        "\n",
        "    def _uct_select(self, node):\n",
        "        \"Select a child of node, balancing exploration & exploitation\"\n",
        "\n",
        "        # All children of node should already be expanded:\n",
        "        assert all(n in self.children for n in self.children[node])\n",
        "\n",
        "        log_N_vertex = math.log(self.N[node])\n",
        "\n",
        "        def uct(n):\n",
        "            \"Upper confidence bound for trees\"\n",
        "            return self.Q[n] / self.N[n] + self.exploration_weight * math.sqrt(\n",
        "                log_N_vertex / self.N[n]\n",
        "            )\n",
        "\n",
        "        return max(self.children[node], key=uct)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXjxTyOILRFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm_EVWOzHqxF",
        "colab_type": "text"
      },
      "source": [
        "### MCTSAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3V_tgrRHsI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MCTSAI(MCTS):\n",
        "  def __init__(self, model_white, model_black,  exploration_weight=1):\n",
        "    #j'initialise avec un model en supplement de ma classe de depart\n",
        "    super().__init__(exploration_weight)\n",
        "    self.model_white = model_white\n",
        "    self.model_black = model_black\n",
        "    self.Q_model = defaultdict(int)\n",
        "    self.all_moves = create_uci_labels()\n",
        "    self.all_moves_dict_str = {move: i for i, move in enumerate(self.all_moves)}\n",
        "    self.all_moves_dict_index = {i: move for i, move in enumerate(self.all_moves)}\n",
        "  \n",
        "  def get_average_reward(self, node):\n",
        "    #return the average of all the collected rewards of a position\n",
        "    number_occurences = self.N[node]\n",
        "    total_rewards = self.Q[node]\n",
        "    return number_occurences / total_rewards\n",
        "\n",
        "  def get_labels(self, node):\n",
        "    #true policy value\n",
        "    next_rewards = {self.children[node].get_last_move(): self.get_average_reward(children) for children in self.children[node]]\n",
        "    policy_probabilities = np.zeros(len(self.all_moves))\n",
        "\n",
        "    for move_str in policy:\n",
        "      move_index = self.all_moves_dict_index[move_str]\n",
        "      policy_probabilities[move_index] = policy[move_index]\n",
        "  \n",
        "    # \"true\" value value\n",
        "    value = self.Q[node] / self.N[node]\n",
        "    return policy_probabilities, value\n",
        "\n",
        "  def _model_select(self, node):\n",
        "      # selectionne un coup choisis par le modele, et \n",
        "      # entraine le modele en fonction de ses \"mauvaises\" predictions\n",
        "      policy_probabilities, value = self.get_labels(node)\n",
        "      #true policy value\n",
        "      policy = [self.Q[children] / self.N[children] for children in self.children[node]]\n",
        "      # \"true\" value value\n",
        "      value = self.Q[node] / self.N[node]\n",
        "      if node.turn == chess.WHITE:\n",
        "        model = self.model_white\n",
        "      else:\n",
        "        model = self.model_black\n",
        "      input_model = node.get_numpy_board() #l'entree de mon modele\n",
        "      # where can I store the probability prediction of next moves?\n",
        "      probs, self.V_model[node] = model.predict(input_model)\n",
        "      legal_moves = list(node.legal_moves())\n",
        "      #cette ligne fait pas mal de choses:\n",
        "      # 1. selectionne seulement les coups legaux\n",
        "      # 2. associe a chaque coup legaux la probabilite predicte par le model\n",
        "      # 3. Sauvegarde tout ca dans un dictionnaire key: coup, valeur: probability\n",
        "      # 4. renormalisation des probabilites\n",
        "      policy_predicted = {get_move_str(i): x for i, x in enumerate(probs) if get_move_str(i) in legal_moves}\n",
        "      sum_probabilities = np.sum(list(policy.values())) #should be less than 1 because a subset\n",
        "      policy_predicted = {k:(v/sum_probabilities + self.exploration_weight * math.sqrt(log_N_vertex / self.N[n]))  for  k,v in policy_predicted.items()}\n",
        "      #remarque: attention a conserver l'ordre dans le dictionnaire? Ok python > 3.6\n",
        "      move_sampled = np.random.choice(list(policy.keys()),p=list(policy.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfYQWzyJHshi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "# import keras #already present in tensorflow 2.0 whitout bugs\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Flatten, Dense\n",
        "from keras.models import Model\n",
        "def model_moves_and_score():\n",
        "    NUMBER_FILTERS = 16\n",
        "    shape_board = (8,8,18)\n",
        "    number_moves = len(all_moves)\n",
        "    #take a numpy array representing a board as input, and who plays first, and then return\n",
        "    # the value of a game, number between -1 and 1\n",
        "    board = Input(shape=(shape_board))\n",
        "    features = Conv2D(filters=NUMBER_FILTERS,kernel_size=3,activation=\"relu\", padding=\"same\")(board)\n",
        "    features = BatchNormalization()(features)\n",
        "    features = Conv2D(filters=NUMBER_FILTERS, kernel_size=3,activation=\"relu\", padding=\"same\")(features)\n",
        "    features = BatchNormalization()(features)\n",
        "    features = Conv2D(filters=2, kernel_size=3,activation=\"relu\", padding=\"same\")(features)\n",
        "    print(features.shape)\n",
        "    features = Flatten()(features)\n",
        "    value_prediction = Dense(1, activation=\"tanh\")(features) \n",
        "    policy_prediction = Dense(number_moves, activation=\"sigmoid\")(features)\n",
        "    model = Model(board, [policy_prediction, value_prediction])\n",
        "    # il y a deux fonctions de pertes car 2 outputs: un vecteur de probabilites \n",
        "    #pour tout les coups possibles ainsi que le score de la position (qui definit qui a l'avantage, les blancs ou les noirs)\n",
        "    model.compile(loss=[\"categorical_crossentropy\", \"mean_squared_error\"], optimizer=\"Adam\")\n",
        "    model.summary()\n",
        "    return model\n",
        "model_black = model_moves_and_score()\n",
        "model_white = model_moves_and_score()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}